# Multi-Provider Token Counting Guide

Your app supports **12 different AI providers**! Each handles token counting differently. Here's the complete breakdown:

## **Provider-Specific Token Reporting**

### **1. OpenAI Models** üü¢ (Full Auto-Counting)
**Models**: `gpt-4o`, `gpt-4-turbo`, `o1`, etc.
```json
{
  "usage": {
    "prompt_tokens": 1500,        // ‚úÖ Includes file content
    "completion_tokens": 200,     // ‚úÖ Response tokens
    "total_tokens": 1700         // ‚úÖ Auto-calculated
  }
}
```
**File Support**: ‚úÖ PDF, Images (tokens auto-counted)

### **2. Anthropic (Claude)** üü¢ (Full Auto-Counting)  
**Models**: `claude-3-7-sonnet`, `claude-3.5`, etc.
```json
{
  "usage": {
    "input_tokens": 1500,        // ‚úÖ Includes file content
    "output_tokens": 200,        // ‚úÖ Response tokens  
    "total_tokens": 1700         // ‚úÖ Auto-calculated
  }
}
```
**File Support**: ‚úÖ PDF, Images (tokens auto-counted)

### **3. Meta (Llama)** üü° (Partial Auto-Counting)
**Models**: `meta-llama/llama-4-maverick`
```json
{
  "usage": {
    "prompt_tokens": 1500,       // ‚úÖ Usually provided
    "completion_tokens": 200,    // ‚úÖ Usually provided
    "total_tokens": 1700        // ‚ö†Ô∏è Sometimes missing
  }
}
```
**File Support**: ‚úÖ Images only (basic token counting)

### **4. Google Models** üü° (Inconsistent)
**Models**: `gemini-pro`, `gemma-3-27b-it`
```json
{
  "usage": {
    "promptTokenCount": 1500,    // ‚úÖ Different field names
    "candidatesTokenCount": 200, // ‚úÖ Different field names
    "totalTokenCount": 1700     // ‚ö†Ô∏è Sometimes missing
  }
}
```
**File Support**: ‚úÖ Images, Documents (limited token info)

### **5. Cohere** üî¥ (Manual Counting Required)
**Models**: `cohere/command-r-plus`
```json
{
  "meta": {
    "tokens": {
      "input_tokens": 1500,      // ‚úÖ Basic counting
      "output_tokens": 200       // ‚úÖ Basic counting
      // ‚ùå No file token breakdown
    }
  }
}
```
**File Support**: ‚úÖ PDF (but file tokens NOT separated)

### **6. DeepSeek** üü° (Basic Auto-Counting)
**Models**: `deepseek/deepseek-r1`
```json
{
  "usage": {
    "prompt_tokens": 1500,       // ‚úÖ Usually provided
    "completion_tokens": 200,    // ‚úÖ Usually provided
    "total_tokens": 1700        // ‚úÖ Usually provided
  }
}
```
**File Support**: ‚úÖ Multiple formats (basic token info)

### **7. Qwen** üü° (Basic Auto-Counting)
**Models**: `Qwen/Qwen3-235B-A22B-fp8-tput`
```json
{
  "usage": {
    "input_tokens": 1500,        // ‚úÖ Usually provided
    "output_tokens": 200,        // ‚úÖ Usually provided
    "total_tokens": 1700        // ‚úÖ Usually provided
  }
}
```
**File Support**: ‚ùå No file support configured

### **8. xAI (Grok)** üü¢ (Good Auto-Counting)
**Models**: `x-ai/grok-3-beta`  
```json
{
  "usage": {
    "prompt_tokens": 1500,       // ‚úÖ Includes file content
    "completion_tokens": 200,    // ‚úÖ Response tokens
    "total_tokens": 1700        // ‚úÖ Auto-calculated
  }
}
```
**File Support**: ‚úÖ Images, Documents (good token counting)

### **9. Mistral** üî¥ (OCR Only - Special Case)
**Models**: `mistral/mistral-ocr-latest`
```json
{
  "usage": {
    "prompt_tokens": 0,          // ‚ö†Ô∏è OCR models don't count input
    "completion_tokens": 5000,   // ‚úÖ Large output (extracted text)
    "total_tokens": 5000        // ‚úÖ Total provided
  }
}
```
**File Support**: ‚úÖ PDF, Images (OCR extraction, not analysis)

### **10. Perplexity** üü° (Web Search Focus)
**Models**: `perplexity/sonar-pro`
```json
{
  "usage": {
    "prompt_tokens": 1500,       // ‚úÖ Basic counting
    "completion_tokens": 200,    // ‚úÖ Basic counting
    "total_tokens": 1700        // ‚úÖ Basic counting
  }
}
```
**File Support**: ‚úÖ Images only (web search focused)

### **11. ZhiPu (GLM)** üü° (Basic Auto-Counting)
**Models**: `zhipu/glm-4.5`
```json
{
  "usage": {
    "prompt_tokens": 1500,       // ‚úÖ Usually provided
    "completion_tokens": 200,    // ‚úÖ Usually provided  
    "total_tokens": 1700        // ‚úÖ Usually provided
  }
}
```
**File Support**: ‚úÖ Multiple formats (basic token info)

## **Universal Token Extraction Code**

Here's code that works across ALL providers:

```kotlin
/**
 * Universal token extractor that works with all AI providers
 */
object UniversalTokenCounter {
    
    fun extractTokenUsage(response: AimlApiResponse, modelId: String): TokenUsage {
        val usage = response.usage
        if (usage == null) {
            Timber.w("‚ö†Ô∏è No token usage provided by model: $modelId")
            return TokenUsage(0, 0, 0)
        }
        
        val provider = getProviderFromModelId(modelId)
        
        return when (provider) {
            "openai" -> extractOpenAITokens(usage)
            "anthropic" -> extractAnthropicTokens(usage) 
            "google" -> extractGoogleTokens(usage)
            "meta" -> extractMetaTokens(usage)
            "cohere" -> extractCohereTokens(usage)
            "deepseek" -> extractDeepSeekTokens(usage)
            "qwen" -> extractQwenTokens(usage)
            "xai" -> extractXAITokens(usage)
            "mistral" -> extractMistralTokens(usage)
            "perplexity" -> extractPerplexityTokens(usage)
            "zhipu" -> extractZhiPuTokens(usage)
            else -> extractGenericTokens(usage) // Fallback
        }
    }
    
    private fun extractOpenAITokens(usage: AimlApiResponse.Usage): TokenUsage {
        return TokenUsage(
            inputTokens = usage.promptTokens ?: 0,
            outputTokens = usage.completionTokens ?: 0, 
            totalTokens = usage.totalTokens ?: 0
        )
    }
    
    private fun extractAnthropicTokens(usage: AimlApiResponse.Usage): TokenUsage {
        return TokenUsage(
            inputTokens = usage.inputTokens ?: usage.promptTokens ?: 0,
            outputTokens = usage.outputTokens ?: usage.completionTokens ?: 0,
            totalTokens = usage.totalTokens ?: 0
        )
    }
    
    private fun extractGoogleTokens(usage: AimlApiResponse.Usage): TokenUsage {
        // Google uses different field names
        val inputTokens = usage.promptTokens ?: usage.inputTokens ?: 0
        val outputTokens = usage.completionTokens ?: usage.outputTokens ?: usage.generatedTokens ?: 0
        val totalTokens = usage.totalTokens ?: (inputTokens + outputTokens)
        
        return TokenUsage(inputTokens, outputTokens, totalTokens)
    }
    
    private fun extractGenericTokens(usage: AimlApiResponse.Usage): TokenUsage {
        // Fallback for any provider
        val inputTokens = usage.promptTokens ?: usage.inputTokens ?: usage.contextTokens ?: 0
        val outputTokens = usage.completionTokens ?: usage.outputTokens ?: usage.generatedTokens ?: 0
        val totalTokens = usage.totalTokens ?: (inputTokens + outputTokens)
        
        return TokenUsage(inputTokens, outputTokens, totalTokens)
    }
    
    private fun getProviderFromModelId(modelId: String): String {
        return ModelConfigManager.getConfig(modelId)?.provider ?: "unknown"
    }
    
    data class TokenUsage(
        val inputTokens: Int,
        val outputTokens: Int,  
        val totalTokens: Int
    ) {
        fun hasValidData(): Boolean = totalTokens > 0 || (inputTokens > 0 && outputTokens > 0)
        
        fun getDisplayText(): String {
            return when {
                totalTokens > 0 -> "üìä ${inputTokens.format()} in ‚Ä¢ ${outputTokens.format()} out ‚Ä¢ ${totalTokens.format()} total"
                inputTokens > 0 || outputTokens > 0 -> "üìä ${inputTokens.format()} in ‚Ä¢ ${outputTokens.format()} out"
                else -> "üìä No token data"
            }
        }
        
        private fun Int.format(): String {
            return when {
                this >= 1_000_000 -> "${this / 1_000_000}M"
                this >= 1_000 -> "${this / 1_000}k"
                else -> toString()
            }
        }
    }
}
```

## **File Token Support Matrix**

| Provider | PDF Support | Token Counting | File Token Separation |
|----------|-------------|----------------|----------------------|
| OpenAI | ‚úÖ Full | ‚úÖ Automatic | ‚úÖ Included in prompt_tokens |
| Anthropic | ‚úÖ Full | ‚úÖ Automatic | ‚úÖ Included in input_tokens |
| xAI (Grok) | ‚úÖ Good | ‚úÖ Automatic | ‚úÖ Included in prompt_tokens |
| DeepSeek | ‚úÖ Basic | ‚úÖ Basic | ‚ö†Ô∏è Limited breakdown |
| ZhiPu | ‚úÖ Basic | ‚úÖ Basic | ‚ö†Ô∏è Limited breakdown |
| Cohere | ‚úÖ PDF Only | ‚ö†Ô∏è Basic | ‚ùå No separation |
| Google | ‚ö†Ô∏è Limited | ‚ö†Ô∏è Inconsistent | ‚ùå No separation |
| Meta | ‚ö†Ô∏è Images Only | ‚ö†Ô∏è Basic | ‚ùå No separation |
| Perplexity | ‚ö†Ô∏è Images Only | ‚úÖ Basic | ‚ùå No separation |
| Mistral | ‚úÖ OCR Only | ‚ö†Ô∏è Output Only | ‚ùå Special case |
| Qwen | ‚ùå None | ‚úÖ Basic | ‚ùå No files |

## **Implementation Strategy**

```kotlin
// In your message handler
fun handleApiResponse(response: AimlApiResponse, modelId: String, hasFiles: Boolean) {
    // Extract tokens universally
    val tokenUsage = UniversalTokenCounter.extractTokenUsage(response, modelId)
    
    if (tokenUsage.hasValidData()) {
        // Update UI
        updateTokenDisplay(tokenUsage.getDisplayText())
        
        // Store usage
        TokenUsageStorage.store(tokenUsage, modelId, hasFiles)
        
        // Analyze file impact (only for supported providers)
        if (hasFiles && supportsFileTokenBreakdown(modelId)) {
            analyzeFileTokenImpact(tokenUsage, modelId)
        }
    } else {
        Timber.w("‚ö†Ô∏è Model $modelId provided no token usage data")
        updateTokenDisplay("üìä No token data")
    }
}

private fun supportsFileTokenBreakdown(modelId: String): Boolean {
    val provider = ModelConfigManager.getConfig(modelId)?.provider
    return provider in listOf("openai", "anthropic", "xai")
}
```

## **Key Takeaways** üéØ

1. **OpenAI, Anthropic, xAI**: Full automatic counting including file tokens
2. **Others**: Basic counting, manual estimation may be needed for files  
3. **Universal Approach**: Use the extraction code above to handle all providers
4. **File Support**: Only some providers separate file tokens from message tokens
5. **Fallback Strategy**: Always have generic token extraction for unknown providers

**Bottom line**: You still don't need to manually count most tokens, but the quality and detail of token reporting varies significantly across providers! üìä